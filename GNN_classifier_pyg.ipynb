{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### INSTALATION #######\n",
    "\n",
    "!pip uninstall torch -y\n",
    "!pip install torch==1.13.1\n",
    "# !pip uninstall torch-scatter -y\n",
    "# !pip uninstall torch-sparse -y\n",
    "# !pip uninstall pyg-lib -y\n",
    "# !pip uninstall git+https://github.com/pyg-team/pytorch_geometric.git -y\n",
    "# !pip uninstall sentence_transformers -y\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "!pip install pyg-lib -f https://data.pyg.org/whl/nightly/torch-${TORCH}.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install pyarrow fastparquet\n",
    "!pip install transformers\n",
    "!pip install lightfm\n",
    "!pip install memory-profiler\n",
    "!pip install imbalanced-learn\n",
    "!pip install xgboost\n",
    "!pip install gensim nltk\n",
    "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "# !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "# !pip install sentence_transformers==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seankhatiri/Fakeclub/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###### IMPORT #######\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "# from neo4j import GraphDatabase\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import re\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing, feature_extraction, model_selection\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from memory_profiler import profile\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch import Tensor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SETUP ARGS ###########\n",
    "possible_experiments = {\n",
    "    0: 'full',\n",
    "    1: 'effectiveness_emb_size',\n",
    "    2: 'effectiveness_emb_epoch',\n",
    "    3: 'ablation',\n",
    "    4: 'TFIDF',\n",
    "    5: 'SBERT',\n",
    "    6: 'word2vec',\n",
    "    7: 'scalability',\n",
    "}\n",
    "experiment = possible_experiments[5]\n",
    "\n",
    "possible_modes = ['debug', 'experiment']\n",
    "mode = possible_modes[0]\n",
    "\n",
    "model_variants = ['random', 'ml', 'gnn']\n",
    "len_interactions_to_consider = 50000\n",
    "# model_variant_eval = model_variants[1]\n",
    "\n",
    "dataset_mode = 'review'\n",
    "embedding_size = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### NEURAL CLASSIFIER ON SMALL LABELED DATASET #############\n",
    "df = pd.read_csv('dataset/Labelled Yelp Dataset.csv')\n",
    "\n",
    "df_label_1 = df[df['Label'] == 1]\n",
    "df_label_minus_1 = df[df['Label'] == -1]\n",
    "num_samples = min(len(df_label_1), len(df_label_minus_1))\n",
    "df_label_1_sampled = df_label_1.sample(n=num_samples, random_state=42)\n",
    "df_label_minus_1_sampled = df_label_minus_1.sample(n=num_samples, random_state=42)\n",
    "balanced_df = pd.concat([df_label_1_sampled, df_label_minus_1_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "balanced_df['Label'] = balanced_df['Label'].apply(lambda x: 0 if x == -1 else 1)\n",
    "\n",
    "def prepare_dataset(df, tfidf_vectorizer):\n",
    "    # 1. Label encoding for products\n",
    "    product_encoder = LabelEncoder()\n",
    "    df['encoded_product'] = product_encoder.fit_transform(df['Product_id'])\n",
    "\n",
    "    review_embeddings = tfidf_vectorizer.fit_transform(df['Review']).toarray()\n",
    "\n",
    "    # Convert to torch tensors and move to appropriate device\n",
    "    product_ids = torch.tensor(df['encoded_product'].values, dtype=torch.long)\n",
    "    review_texts = torch.tensor(review_embeddings, dtype=torch.float)\n",
    "    labels = torch.tensor(df['Label'].values, dtype=torch.long)\n",
    "\n",
    "    # Create a DataLoader for batching\n",
    "    dataset = TensorDataset(product_ids, review_texts, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=2048, shuffle=True)\n",
    "    return dataloader, product_encoder, review_embeddings\n",
    "\n",
    "temp_df = balanced_df[:-1]\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "df_train, df_test = train_test_split(temp_df, test_size=0.1, random_state=42)\n",
    "train_dataloader, train_product_encoder, train_review_embeddings = prepare_dataset(df_train, tfidf_vectorizer)\n",
    "test_dataloader, _, _ = prepare_dataset(df_test, tfidf_vectorizer)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, num_products, review_embedding_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        \n",
    "        embedding_dim = 32  # arbitrary size, adjust as needed\n",
    "        \n",
    "        # User and Product Embeddings\n",
    "        # self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.product_embedding = nn.Embedding(num_products, embedding_dim)\n",
    "        \n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(embedding_dim + review_embedding_dim, 128)  # combining user, product and text embeddings\n",
    "        self.fc2 = nn.Linear(128, 2)  # output 2 values for binary classification\n",
    "        \n",
    "    def forward(self, product_ids, review_embeddings):\n",
    "        # user_embed = self.user_embedding(user_ids)\n",
    "        product_embed = self.product_embedding(product_ids)\n",
    "        \n",
    "        # Concatenate the embeddings\n",
    "        combined = torch.cat([product_embed, review_embeddings], dim=1)\n",
    "        \n",
    "        # Pass through the linear layers\n",
    "        x = F.relu(self.fc1(combined))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleClassifier(len(train_product_encoder.classes_), train_review_embeddings.shape[1])\n",
    "model = model.to(device)  # Move the model to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# def compute_average_loss(model, train_dataloader, criterion):\n",
    "#     total_loss = 0.0\n",
    "#     total_samples = 0\n",
    "\n",
    "#     with torch.no_grad():  # Ensure no gradients are computed\n",
    "#         for batch_idx, (products, reviews, labels) in enumerate(train_dataloader):\n",
    "#             outputs = model(products, reviews)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             total_loss += loss.item() * labels.size(0)\n",
    "#             total_samples += labels.size(0)\n",
    "\n",
    "#     return total_loss / total_samples\n",
    "\n",
    "# # Compute and print the loss before training\n",
    "# initial_loss = compute_average_loss(model, train_dataloader, criterion)\n",
    "# print(f\"Initial Loss (Untrained): {initial_loss:.4f}\")\n",
    "\n",
    "def calculate_class_accuracies(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct_class_1 = ((predicted == 1) & (labels == 1)).sum().item()\n",
    "    correct_class_fake = ((predicted == 0) & (labels == 0)).sum().item()\n",
    "    total_class_1 = (labels == 1).sum().item()\n",
    "    total_class_minus_1 = (labels == 0).sum().item()\n",
    "\n",
    "    acc_class_1 = correct_class_1 / total_class_1  if total_class_1 != 0 else 0\n",
    "    acc_class_fake = correct_class_fake / total_class_minus_1 if total_class_minus_1 != 0 else 0\n",
    "\n",
    "    return acc_class_1, acc_class_fake\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    return correct / labels.size(0)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (products, reviews, labels) in enumerate(train_dataloader):\n",
    "        products, reviews, labels = products.to(device), reviews.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(products, reviews)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    total_accuracy_class_1 = 0\n",
    "    total_accuracy_class_fake = 0\n",
    "    with torch.no_grad():\n",
    "        for val_products, val_reviews, val_labels in test_dataloader:\n",
    "            val_products, val_reviews, val_labels = val_products.to(device), val_reviews.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_products, val_reviews)\n",
    "            acc_class_1, acc_class_fake = calculate_class_accuracies(val_outputs, val_labels)\n",
    "            total_accuracy_class_1 += acc_class_1\n",
    "            total_accuracy_class_fake += acc_class_fake\n",
    "            # total_accuracy += calculate_accuracy(val_outputs, val_labels)\n",
    "\n",
    "\n",
    "    avg_accuracy_class_1 = total_accuracy_class_1 / len(test_dataloader)\n",
    "    avg_accuracy_class_fake = total_accuracy_class_fake / len(test_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, \\\n",
    "        Validation Accuracy for Class Not Fake: {avg_accuracy_class_1*100:.2f}%, \\\n",
    "        Validation Accuracy for Class Fake: {avg_accuracy_class_fake*100:.2f}%\")\n",
    "\n",
    "    # avg_accuracy = total_accuracy / len(test_dataloader)\n",
    "    # print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Validation Accuracy: {avg_accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### LABEL NEW DATASET WITH NEURAL CLASSIFIER #########\n",
    "def predict_labels(df, model, product_encoder, tfidf_vectorizer):\n",
    "    model.eval()\n",
    "\n",
    "    df['encoded_business'] = product_encoder.transform(df['business_id'])\n",
    "    review_embeddings = tfidf_vectorizer.transform(df['text']).toarray()\n",
    "\n",
    "    product_ids = torch.tensor(df['encoded_business'].values, dtype=torch.long)\n",
    "    review_texts = torch.tensor(review_embeddings, dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(product_ids, review_texts)\n",
    "    dataloader = DataLoader(dataset, batch_size=2048)\n",
    "\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for products, reviews in dataloader:\n",
    "            products, reviews = products.to(device), reviews.to(device)\n",
    "            \n",
    "            outputs = model(products, reviews)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    df['Predicted_Label'] = predicted_labels\n",
    "\n",
    "    return df\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "unlabeled_df = pd.read_parquet('dataset/yelp_dataset_ver2_small.parquet')\n",
    "def encode_business_id(unlabeled_df):\n",
    "    encoder = LabelEncoder()\n",
    "    unlabeled_df['business_id'] = encoder.fit_transform(unlabeled_df['business_id'])\n",
    "    return unlabeled_df\n",
    "\n",
    "unlabeled_df = encode_business_id(unlabeled_df)\n",
    "updated_df = predict_labels(unlabeled_df, model, train_product_encoder, tfidf_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ARCHIVED CREATE DATASET FOR GNN CLASSIFIER #############\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class CustomDataset:\n",
    "     \n",
    "    def __len__(self):\n",
    "        # Assuming one graph for the entire dataset. \n",
    "        # If you have multiple graphs, you should return the number of graphs here.\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For this example, I'm assuming you have one big graph for the entire dataset.\n",
    "        # So, regardless of the index, we return the same graph.\n",
    "        # This method needs to be adapted if you have multiple graphs.\n",
    "        data = Data(\n",
    "            x=self.x, \n",
    "            edge_index=torch.tensor(self.edge_index).t().contiguous(), \n",
    "            edge_attr=torch.stack(self.edge_attr),\n",
    "            y=torch.tensor(self.y, dtype=torch.float32)  # Add the edge labels here\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.vectorizer = TfidfVectorizer(max_features=100) # Example to limit features to 100, adjust accordingly\n",
    "        self.vectorizer.fit(df['text'])\n",
    "        self.edge_index, self.edge_attr, self.y = self.dataframe_to_graph()\n",
    "        self.x = self.encode_node_features()\n",
    "        self.y = []\n",
    "\n",
    "    def dataframe_to_graph(self):\n",
    "        \"\"\"\n",
    "        Convert the DataFrame to a graph structure using user_id, friends, business_id, and stars.\n",
    "        \"\"\"\n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        labels = []\n",
    "\n",
    "        # Creating Review edges\n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df), desc='processing review edges'):\n",
    "            user_id = row['user_id']\n",
    "            business_id = row['business_id'] + max(self.df['user_id']) # Ensuring unique node IDs by offsetting business IDs\n",
    "            edge_index.append((user_id, business_id))\n",
    "            \n",
    "            # Extract TFIDF embedding for the text review\n",
    "            tfidf_vector = self.get_tfidf_embedding(row['text']) # A method to be implemented\n",
    "            edge_attr.append(torch.tensor([row['stars']] + tfidf_vector))\n",
    "            labels.append(row['label'])\n",
    "\n",
    "        # Creating Friendship edges\n",
    "        for _, row in tqdm(self.df.iterrows(), total=len(self.df), desc='processing social edges'):\n",
    "            user_id = row['user_id']\n",
    "            for friend_id in row['friends']:\n",
    "                edge_index.append((user_id, friend_id))\n",
    "                edge_attr.append(torch.tensor([0] + [0]*len(tfidf_vector))) # No specific feature for friendship edge\n",
    "                labels.append(1)\n",
    "\n",
    "        # Convert edge_index to a torch tensor of shape [2, num_edges]\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return edge_index, edge_attr\n",
    "\n",
    "    def get_tfidf_embedding(self, text):\n",
    "        \"\"\"\n",
    "        Convert a given text to its TFIDF embedding.\n",
    "        \"\"\"\n",
    "        # Assuming a global vectorizer, or you can initialize and fit one here.\n",
    "        vectorized = self.vectorizer.transform([text])\n",
    "        return vectorized.toarray().flatten().tolist()\n",
    "\n",
    "    def encode_node_features(self):\n",
    "        \"\"\"\n",
    "        Since nodes don't have specific features other than their IDs,\n",
    "        a simple one-hot encoding (or an embedding lookup) can be used.\n",
    "        However, for simplicity, we will use node IDs as features here.\n",
    "        \"\"\"\n",
    "        # num_nodes = max(self.df['user_id']) + max(self.df['business_id'])\n",
    "        num_nodes = max(self.df['user_id']) + 1 + max(self.df['business_id'])\n",
    "        x = torch.arange(num_nodes).float().unsqueeze(-1)\n",
    "        return x\n",
    "    \n",
    "    def one_hot_encode_star(self, star, max_value=5):\n",
    "        \"\"\"\n",
    "        One-hot encode the star rating.\n",
    "        \"\"\"\n",
    "        one_hot = [0] * (max_value + 1) # +1 because rating starts from 0\n",
    "        one_hot[star] = 1\n",
    "        return one_hot\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(train_df)\n",
    "val_dataset = CustomDataset(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ARCHIVED TRAIN GNN CLASSIFIER V1 #############\n",
    "import torch.optim as optim \n",
    "\n",
    "# 2. Define the GNN Model (from the earlier provided code)\n",
    "class GraphSAGEClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super(GraphSAGEClassifier, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 3. Define loss and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGEClassifier(1, 128, 2).to(device)  # Assuming binary classification (labels 0 and 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Train the model\n",
    "def train(dataset, model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset.x.to(device), dataset.edge_index.to(device))\n",
    "    \n",
    "    # Assuming that you've stored labels for the nodes involved in the reviews.\n",
    "    mask = [i for i, row in dataset.df.iterrows()]  # Only consider nodes involved in reviews.\n",
    "    loss = criterion(out[mask], torch.tensor(dataset.df['label'].values).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# 5. Evaluate the model\n",
    "def evaluate(dataset, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(dataset.x.to(device), dataset.edge_index.to(device))\n",
    "        mask = [i for i, row in dataset.df.iterrows()]\n",
    "        pred = out[mask].max(1)[1]\n",
    "        correct = pred.eq(torch.tensor(dataset.df['label'].values).to(device)).sum().item()\n",
    "        return correct / len(mask)\n",
    "\n",
    "# Training loop\n",
    "epochs = 20\n",
    "best_acc = 0\n",
    "best_loss = 10000\n",
    "gnn_model = ''\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train(train_dataset, model)\n",
    "    train_acc = evaluate(train_dataset, model)\n",
    "    val_acc = evaluate(val_dataset, model)\n",
    "    if val_acc > best_acc: gnn_model = model\n",
    "    if val_acc == best_acc:\n",
    "        if loss < best_loss: gnn_model = model\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ARCHIVED TRAIN GNN CLASSIFIER V2 #############\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "class EdgeClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, edge_feature_size):\n",
    "        super(EdgeClassifier, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Classifier that operates on the concatenation of both node embeddings + edge features\n",
    "        self.classifier = torch.nn.Linear(2 * hidden_channels + edge_feature_size, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x1 = self.conv1(x, edge_index)\n",
    "        x1 = F.relu(x1)\n",
    "        \n",
    "        x2 = self.conv2(x1, edge_index)\n",
    "        x2 = F.relu(x2)\n",
    "\n",
    "        # Concatenating the embeddings of source and target nodes along with edge attributes\n",
    "        src, tgt = edge_index\n",
    "        edge_embeddings = torch.cat([x2[src], x2[tgt], edge_attr], dim=1)\n",
    "        return torch.sigmoid(self.classifier(edge_embeddings))\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data.edge_index = data.edge_index.t().contiguous()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        loss = criterion(out.view(-1), data.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = EdgeClassifier(in_channels=1, hidden_channels=64, edge_feature_size=len(train_dataset.edge_attr[0]))\n",
    "criterion = torch.nn.BCELoss()  # Binary cross entropy for edge classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    loss = train(model, loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100)\n",
    "train_tfidf = vectorizer.fit_transform(train_df['text']).toarray()\n",
    "\n",
    "# Training the logistic regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_tfidf, train_df['label'])\n",
    "\n",
    "test_tfidf = vectorizer.transform(test_df['text']).toarray()\n",
    "lr_preds = lr.predict(test_tfidf)\n",
    "\n",
    "test_dataset = CustomDataset(test_df)\n",
    "gnn_out = gnn_model(test_dataset.x.to(device), test_dataset.edge_index.to(device))\n",
    "gnn_preds = gnn_out.max(1)[1].cpu().numpy()\n",
    "\n",
    "lr_accuracy = accuracy_score(test_df['label'], lr_preds)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "gnn_accuracy = accuracy_score(test_df['label'], gnn_preds)\n",
    "print(f\"GNN Accuracy: {gnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49818, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######### START BELOW ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49818, 768])\n",
      "number of unique users 42936\n",
      "number of unique items 8026\n"
     ]
    }
   ],
   "source": [
    "#### DATA LOADER ####\n",
    "def data_loader(ratings_df):\n",
    "    unique_user_id = ratings_df['userId'].unique()\n",
    "    unique_user_id = pd.DataFrame(data={\n",
    "        'userId': unique_user_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "    })\n",
    "\n",
    "    unique_item_id = ratings_df['itemId'].unique()\n",
    "    unique_item_id = pd.DataFrame(data={\n",
    "        'itemId': unique_item_id,\n",
    "        'mappedID': pd.RangeIndex(len(unique_item_id)),\n",
    "    })\n",
    "\n",
    "    ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                                left_on='userId', right_on='userId', how='left')\n",
    "    ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "    ratings_item_id = pd.merge(ratings_df['itemId'], unique_item_id,\n",
    "                                left_on='itemId', right_on='itemId', how='left')\n",
    "    ratings_item_id = torch.from_numpy(ratings_item_id['mappedID'].values)\n",
    "    edge_index_user_to_item = torch.stack([ratings_user_id, ratings_item_id], dim=0)\n",
    "\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item\n",
    "\n",
    "def review_loader():\n",
    "    df = pd.read_parquet('dataset/labeled_yelp_dataset_ver2_small.parquet')\n",
    "    def encode_id(df):\n",
    "        business_encoder = LabelEncoder()\n",
    "        df['business_id'] = business_encoder.fit_transform(df['business_id'])\n",
    "        user_encoder = LabelEncoder()\n",
    "        mapping = dict(zip(df['user_id'] ,user_encoder.fit_transform(df['user_id'])))\n",
    "        df['user_id'] = user_encoder.fit_transform(df['user_id'])\n",
    "        def map_friends(friends_str, mapping_dict):\n",
    "            return [mapping_dict[friend] for friend in friends_str.split(',') if friend in mapping_dict]\n",
    "\n",
    "        df['friends'] = df['friends'].apply(map_friends, mapping_dict=mapping)\n",
    "        return df\n",
    "\n",
    "    df = encode_id(df)\n",
    "    df = df[['user_id', 'friends', 'business_id', 'stars', 'text', 'label', 'name']]\n",
    "    df = df.rename(columns={\n",
    "        'user_id': 'user',\n",
    "        'business_id': 'item',\n",
    "        'stars': 'rating',\n",
    "        'text': 'review_text',\n",
    "        'name': 'business_name',\n",
    "    })\n",
    "    df = df.drop_duplicates(subset=['user', 'item'], keep='first')\n",
    "\n",
    "    items_ratings_df = df[:len_interactions_to_consider] if mode == 'debug' else df #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "    # ########## SPARCITY EXPERIMENT ###########\n",
    "    # def calculate_sparcity_value(df):\n",
    "    #     num_users = df['user'].nunique()\n",
    "    #     num_items = df['item'].nunique()\n",
    "    #     num_interactions = len(df)\n",
    "    #     total_possible_interactions = num_users * num_items / 100\n",
    "    #     sparsity = 1 - (num_interactions / total_possible_interactions)\n",
    "    #     return sparsity\n",
    "    # def filter_interactions(df, column, k):\n",
    "    #     valid_entries = df[column].value_counts()\n",
    "    #     valid_entries = valid_entries[valid_entries >= k]\n",
    "    #     df = df[df[column].isin(valid_entries.index)]\n",
    "    #     print(f'{column} sparcity value is:', calculate_sparcity_value(df))\n",
    "    #     return df\n",
    "    # if experiment == 'usparsity':\n",
    "    #     u = 1\n",
    "    #     items_ratings_df = filter_interactions(items_ratings_df, 'user', u)\n",
    "    # elif experiment == 'isparsity':\n",
    "    #     i = 20\n",
    "    #     items_ratings_df = filter_interactions(items_ratings_df, 'item', i)\n",
    "\n",
    "    items_df = {}\n",
    "    items_df['business_name'] = items_ratings_df['business_name'].unique()\n",
    "    items_df['itemId'], unique_names = pd.factorize(items_df['business_name'])\n",
    "    # items_df['itemId'] = items_df['itemId'] + 1 #TODO test commenting this line didn't breal anything\n",
    "    items_df = pd.DataFrame(items_df, columns=['itemId', 'business_name'])\n",
    "\n",
    "    # We have edge feature\n",
    "    def get_edge_feat_sbert(df):\n",
    "        def preprocess_text(text):\n",
    "            # TODO\n",
    "            return text\n",
    "\n",
    "        preprocessed_review_texts = [preprocess_text(review_text[:512]) for review_text in df['review_text']]\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "        model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "        device = torch.device(\"cpu\") #\"cuda\" if torch.cuda.is_available() else \"cpu\") # NOT enough GPU memory\n",
    "        model = model.to(device)\n",
    "        inputs = tokenizer(preprocessed_review_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        review_text_feat = embeddings\n",
    "        # TODO: calculate the stars embedding, maybe concatenate to review_text_emb as final edge_feat\n",
    "        edge_feat = None\n",
    "        \n",
    "        return edge_feat\n",
    "    \n",
    "    def get_edge_feat_tfidf(df):\n",
    "        #TODO: maybe fit a TFIDF, return the top_keywords for each review_text, create the dummy vector of [len(reviews), unique_top_keywords]\n",
    "        pass\n",
    "\n",
    "    def get_edge_feat_word2vec(df):\n",
    "        #TODO:\n",
    "        sentences = df['review_text'].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "        model = Word2Vec(sentences=sentences, vector_size=embedding_size, window=5, min_count=1, workers=4)\n",
    "        model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "        # Average the word vectors for each sentence for our dataset\n",
    "        def get_avg_vector(words, model, num_features):\n",
    "            feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "            nwords = 0.\n",
    "            vocabulary = set(model.wv.index_to_key)\n",
    "\n",
    "            for word in words:\n",
    "                if word in vocabulary: \n",
    "                    nwords = nwords + 1.\n",
    "                    feature_vector = np.add(feature_vector, model.wv[word])\n",
    "            \n",
    "            if nwords:\n",
    "                feature_vector = np.divide(feature_vector, nwords)\n",
    "            return feature_vector\n",
    "\n",
    "        edge_feat = np.array(sentences.apply(lambda x: get_avg_vector(x, model, embedding_size)).tolist())\n",
    "        return edge_feat\n",
    "    \n",
    "    ########### SBERT EXPERIMENT ###########\n",
    "    if experiment == 'TFIDF':\n",
    "        #TODO\n",
    "        edge_feat = None\n",
    "        # edge_feat = get_edge_feat_tfidf(df)\n",
    "        # edge_feat = np.load('tfidf_embeddings_full.npy')\n",
    "        edge_feat = torch.from_numpy(edge_feat[:len(df)]).to(torch.float)\n",
    "    elif experiment == 'word2vec':\n",
    "        # edge_feat = get_edge_feat_word2vec(df)\n",
    "        edge_feat = np.load('word2vec_embeddings_full.npy')\n",
    "        edge_feat = torch.from_numpy(edge_feat[:len(df)]).to(torch.float)\n",
    "    elif experiment == 'SBERT':\n",
    "        # edge_feat = get_edge_feat_sbert(df)\n",
    "        edge_feat = np.load('dataset/sbert_embeddings_full.npy') #np.load('sbert_embeddings_100k.npy')\n",
    "        edge_feat = torch.from_numpy(edge_feat[:len(df)]).to(torch.float)\n",
    "    print(edge_feat.shape)\n",
    "\n",
    "    # print('edge feature tensor shape', edge_feat.shape)\n",
    "    items_ratings_df = items_ratings_df.rename(columns={'user': 'userId', 'item': 'itemId'})\n",
    "    unique_user_id, unique_item_id, edge_index_user_to_item = data_loader(items_ratings_df)\n",
    "    print('number of unique users', len(unique_user_id))\n",
    "    print('number of unique items', len(unique_item_id))\n",
    "    return unique_user_id, unique_item_id, edge_index_user_to_item, items_df, edge_feat, items_ratings_df\n",
    "\n",
    "loaders = {\n",
    "    'review_loader': review_loader,\n",
    "}\n",
    "unique_user_id, unique_item_id, edge_index_user_to_item, item_df, edge_feat, review_ratings_df = loaders[f'{dataset_mode}_loader']()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### LINK CLASS PRED MODEL ##########\n",
    "def train_test_generator(unique_user_id, unique_item_id, review_ratings_df, edge_feat, edge_index_user_to_item):  \n",
    "    if edge_feat == None: edge_feat = torch.zeros((edge_index_user_to_item.shape[1], 768)) # 768 as the SBERT dim output\n",
    "    user_features = torch.zeros((len(review_ratings_df['userId'].unique()), 100))\n",
    "    item_features = torch.zeros((len(review_ratings_df['itemId'].unique()), 100))\n",
    "    data = HeteroData()\n",
    "    data[\"user\"].x = user_features\n",
    "    data[\"item\"].x = item_features\n",
    "    data[\"user\", \"review\", \"item\"].edge_index = edge_index_user_to_item\n",
    "    data[\"user\", \"review\", \"item\"].edge_label = torch.tensor(review_ratings_df['label'], dtype=torch.float)\n",
    "    data[\"user\", \"review\", \"item\"].edge_attr = edge_feat\n",
    "    data = T.ToUndirected()(data)\n",
    "    del data['item', 'rev_review', 'user'].edge_label\n",
    "\n",
    "    train_data, val_data, test_data = T.RandomLinkSplit(\n",
    "        num_val=0.1,\n",
    "        num_test=0.1,\n",
    "        neg_sampling_ratio=0.0,\n",
    "        edge_types=[('user', 'review', 'item')],\n",
    "        rev_edge_types=[('item', 'rev_review', 'user')],\n",
    "    )(data)\n",
    "\n",
    "    # TODO: somehow for train_data, its edge_attr is splited, not sure how\n",
    "    val_data_mask = torch.zeros(data[\"user\", \"review\", \"item\"].edge_index.shape[1], dtype=torch.bool)\n",
    "    test_data_mask = torch.zeros(data[\"user\", \"review\", \"item\"].edge_index.shape[1], dtype=torch.bool)\n",
    "    val_data_edges = set(tuple(edge) for edge in val_data[\"user\", \"review\", \"item\"].edge_label_index.t().tolist())\n",
    "    test_data_edges = set(tuple(edge) for edge in test_data[\"user\", \"review\", \"item\"].edge_label_index.t().tolist())\n",
    "\n",
    "    # Looping through all the edges\n",
    "    for i, edge in tqdm(enumerate(data[\"user\", \"review\", \"item\"].edge_index.t().tolist()), total=len(data[\"user\", \"review\", \"item\"].edge_index.t().tolist()), desc='processing edge_attr splitting for train and test'):\n",
    "        edge_tuple = tuple(edge)\n",
    "        if edge_tuple in val_data_edges:\n",
    "            val_data_mask[i] = True\n",
    "        elif edge_tuple in test_data_edges:\n",
    "            test_data_mask[i] = True\n",
    "\n",
    "    # Assigning edge_attr based on the created masks\n",
    "    val_data[\"user\", \"review\", \"item\"].edge_attr = data[\"user\", \"review\", \"item\"].edge_attr[val_data_mask]\n",
    "    test_data[\"user\", \"review\", \"item\"].edge_attr = data[\"user\", \"review\", \"item\"].edge_attr[test_data_mask]\n",
    "\n",
    "    return data, train_data, val_data, test_data\n",
    "\n",
    "def GNN_recommender(data, train_data):\n",
    "    print('2')\n",
    "\n",
    "    class GNNEncoder(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels, out_channels):\n",
    "            super().__init__()\n",
    "            self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "            self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index).relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "\n",
    "    class EdgeDecoder(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.lin1 = torch.nn.Linear(2 * hidden_channels + train_data['user', 'review', 'item'].edge_attr.shape[1], hidden_channels)\n",
    "            self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "        def forward(self, z_dict, edge_label_index, edge_attr):\n",
    "            row, col = edge_label_index\n",
    "            z = torch.cat([z_dict['user'][row], z_dict['item'][col], edge_attr], dim=-1)\n",
    "\n",
    "            z = self.lin1(z).relu()\n",
    "            z = self.lin2(z)\n",
    "            return z.view(-1)\n",
    "        \n",
    "    class Model(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super().__init__()\n",
    "            self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
    "            self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "            self.decoder = EdgeDecoder(hidden_channels)\n",
    "\n",
    "        def forward(self, x_dict, edge_index_dict, edge_attr, edge_label_index):\n",
    "            z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "            return self.decoder(z_dict, edge_label_index, edge_attr)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Model(hidden_channels=32).to(device)\n",
    "            \n",
    "    # ########## TRAINING ##########\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(\n",
    "            x_dict=train_data.x_dict,\n",
    "            edge_index_dict=train_data.edge_index_dict,\n",
    "            edge_attr=train_data['user', 'review', 'item'].edge_attr,  # Here's where you pass the edge attributes\n",
    "            edge_label_index=train_data['user', 'item'].edge_label_index\n",
    "        )\n",
    "        target = train_data['user', 'item'].edge_label\n",
    "        loss = F.mse_loss(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return float(loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(data):\n",
    "        data = data.to(device)\n",
    "        model.eval()\n",
    "        pred = model(data.x_dict, data.edge_index_dict,\n",
    "                    data['user', 'review', 'item'].edge_attr,\n",
    "                    data['user', 'item'].edge_label_index)\n",
    "        pred = pred.clamp(min=0, max=1)\n",
    "        target = data['user', 'item'].edge_label.float()\n",
    "        rmse = F.mse_loss(pred, target).sqrt()\n",
    "        return float(rmse)\n",
    "\n",
    "\n",
    "    for epoch in range(1, 301):\n",
    "        train_data = train_data.to(device)\n",
    "        loss = train()\n",
    "        train_rmse = test(train_data)\n",
    "        val_rmse = test(val_data)\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_rmse:.4f}, '\n",
    "            f'Val: {val_rmse:.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing edge_attr splitting for train and test: 100%|██████████| 49818/49818 [00:00<00:00, 319335.62it/s]\n"
     ]
    }
   ],
   "source": [
    "########## TRAIN TEST GENERAION ############\n",
    "\n",
    "####### ITEM FEAT ABLATION EXPRIMENT ####### \n",
    "if experiment == 'ablation_edge_feat':\n",
    "    edge_feat = torch.zeros_like(edge_feat)\n",
    "\n",
    "# ####### SOCIAL EDGES ABLEATION EXPERIMENT #######\n",
    "def add_social_edges(edge_index_user_to_item, unique_user_id, unique_item_id, items_ratings_df, item_feat):\n",
    "    unique_item_id = unique_item_id.copy()\n",
    "    # Define the filename where the data will be saved\n",
    "    filename = 'dataset/saved_social_edges_100k.pkl'\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        # If it does, load the data and return it\n",
    "        with open(filename, 'rb') as f:\n",
    "            unique_user_id, unique_item_id_w_users, edge_index_user_to_item, item_feat = pickle.load(f)\n",
    "        print('Data loaded from file')\n",
    "    else:\n",
    "        user_transactions_df = pd.read_parquet('dataset/user_transactions.parquet')\n",
    "        contract_addresses = pd.read_parquet('dataset/contract_addresses.parquet')\n",
    "        contract_set = set(contract_addresses['address'])\n",
    "\n",
    "        # Shifting item_ids\n",
    "        edge_index_user_to_item[1] = edge_index_user_to_item[1] + len(edge_index_user_to_item[0].unique())\n",
    "        unique_item_id['mappedID'] = unique_item_id['mappedID'] + len(edge_index_user_to_item[0].unique())\n",
    "        #Adding user_ids to item_ids since now users can be an item too #TODO if the GNN performance turned to be bad, just add 'to' user addresses to both item_feat and unique_item_ids\n",
    "        unique_item_id_w_users = pd.concat([unique_user_id.rename(columns={'userId': 'entityId'}), unique_item_id.rename(columns={'itemId': 'entityId'})], axis=0)\n",
    "        user_feat = torch.zeros((len(edge_index_user_to_item[0].unique()), item_feat.shape[1]))\n",
    "        item_feat= torch.cat([user_feat, item_feat], dim=0) # Why don't we adding item_feat to user_feat?\n",
    "\n",
    "        # unique_item_id_w_users['type'] = 'user' or 'item'\n",
    "\n",
    "        users = items_ratings_df['userId'].unique()\n",
    "\n",
    "        print('edge index shape before adding social edges:', edge_index_user_to_item.shape)\n",
    "        count = 0\n",
    "        #note there is a 200k constraint, delete it\n",
    "        for i, interaction in tqdm(user_transactions_df.iterrows(), total=len(user_transactions_df)):\n",
    "            if interaction['from'] not in contract_set and interaction['to'] not in contract_set and interaction['from'] in users and interaction['to'] in users:\n",
    "                if interaction['from'] == interaction['to']: continue\n",
    "                from_user_id = unique_item_id_w_users[unique_item_id_w_users['entityId'] == interaction['from']]['mappedID'].iloc[0]\n",
    "                to_user_id = unique_item_id_w_users[unique_item_id_w_users['entityId'] == interaction['to']]['mappedID'].iloc[0]\n",
    "                social_edge = torch.tensor([[from_user_id], \n",
    "                                            [to_user_id]], dtype=torch.int64)\n",
    "                edge_index_user_to_item = torch.cat([edge_index_user_to_item, social_edge], dim=1)\n",
    "                # count += 1\n",
    "                # if count % 5 == 0: break\n",
    "        print('edge index shape after adding social edges:', edge_index_user_to_item.shape)\n",
    "        del user_transactions_df\n",
    "        del contract_addresses\n",
    "        del contract_set\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        #uncomment below\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump((unique_user_id, unique_item_id_w_users, edge_index_user_to_item, item_feat), f)\n",
    "        print('social edges saved to dataset/saved_social_edges_100k.pkl')\n",
    "    \n",
    "    return unique_user_id, unique_item_id_w_users, edge_index_user_to_item, item_feat\n",
    "\n",
    "if experiment == 'add_social_edges':\n",
    "    # uncomment below after debuging\n",
    "    unique_user_id_w_social, unique_item_id_w_social, review_ratings_df_w_social, edge_index_user_to_item_w_social, edge_feat_w_social = add_social_edges(edge_index_user_to_item, unique_user_id, unique_item_id, review_ratings_df, edge_feat)\n",
    "    data, train_data, val_data, test_data = train_test_generator(unique_user_id_w_social, unique_item_id_w_social, edge_feat_w_social, edge_index_user_to_item_w_social)\n",
    "else:\n",
    "    data, train_data, val_data, test_data = train_test_generator(unique_user_id, unique_item_id, review_ratings_df, edge_feat, edge_index_user_to_item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Epoch: 001, Loss: 0.3167, Train: 0.7740, Val: 0.7723\n",
      "Epoch: 002, Loss: 22.1876, Train: 0.7378, Val: 0.7493\n",
      "Epoch: 003, Loss: 0.9276, Train: 0.6208, Val: 0.6225\n",
      "Epoch: 004, Loss: 0.4333, Train: 0.5874, Val: 0.5922\n",
      "Epoch: 005, Loss: 0.3590, Train: 0.5751, Val: 0.5812\n",
      "Epoch: 006, Loss: 0.3335, Train: 0.5743, Val: 0.5785\n",
      "Epoch: 007, Loss: 0.3305, Train: 0.5751, Val: 0.5781\n",
      "Epoch: 008, Loss: 0.3310, Train: 0.5759, Val: 0.5783\n",
      "Epoch: 009, Loss: 0.3318, Train: 0.5765, Val: 0.5785\n",
      "Epoch: 010, Loss: 0.3324, Train: 0.5768, Val: 0.5787\n",
      "Epoch: 011, Loss: 0.3327, Train: 0.5769, Val: 0.5788\n",
      "Epoch: 012, Loss: 0.3329, Train: 0.5769, Val: 0.5787\n",
      "Epoch: 013, Loss: 0.3328, Train: 0.5767, Val: 0.5785\n",
      "Epoch: 014, Loss: 0.3326, Train: 0.5763, Val: 0.5781\n",
      "Epoch: 015, Loss: 0.3322, Train: 0.5759, Val: 0.5777\n",
      "Epoch: 016, Loss: 0.3316, Train: 0.5753, Val: 0.5771\n",
      "Epoch: 017, Loss: 0.3310, Train: 0.5746, Val: 0.5764\n",
      "Epoch: 018, Loss: 0.3302, Train: 0.5738, Val: 0.5756\n",
      "Epoch: 019, Loss: 0.3293, Train: 0.5730, Val: 0.5748\n",
      "Epoch: 020, Loss: 0.3283, Train: 0.5720, Val: 0.5738\n",
      "Epoch: 021, Loss: 0.3272, Train: 0.5710, Val: 0.5728\n",
      "Epoch: 022, Loss: 0.3261, Train: 0.5700, Val: 0.5717\n",
      "Epoch: 023, Loss: 0.3249, Train: 0.5688, Val: 0.5706\n",
      "Epoch: 024, Loss: 0.3236, Train: 0.5677, Val: 0.5694\n",
      "Epoch: 025, Loss: 0.3222, Train: 0.5664, Val: 0.5682\n",
      "Epoch: 026, Loss: 0.3209, Train: 0.5652, Val: 0.5669\n",
      "Epoch: 027, Loss: 0.3194, Train: 0.5639, Val: 0.5656\n",
      "Epoch: 028, Loss: 0.3180, Train: 0.5626, Val: 0.5643\n",
      "Epoch: 029, Loss: 0.3165, Train: 0.5612, Val: 0.5629\n",
      "Epoch: 030, Loss: 0.3150, Train: 0.5599, Val: 0.5616\n",
      "Epoch: 031, Loss: 0.3134, Train: 0.5585, Val: 0.5602\n",
      "Epoch: 032, Loss: 0.3119, Train: 0.5571, Val: 0.5588\n",
      "Epoch: 033, Loss: 0.3103, Train: 0.5556, Val: 0.5574\n",
      "Epoch: 034, Loss: 0.3087, Train: 0.5542, Val: 0.5559\n",
      "Epoch: 035, Loss: 0.3072, Train: 0.5528, Val: 0.5545\n",
      "Epoch: 036, Loss: 0.3056, Train: 0.5514, Val: 0.5530\n",
      "Epoch: 037, Loss: 0.3040, Train: 0.5499, Val: 0.5516\n",
      "Epoch: 038, Loss: 0.3024, Train: 0.5485, Val: 0.5502\n",
      "Epoch: 039, Loss: 0.3009, Train: 0.5471, Val: 0.5487\n",
      "Epoch: 040, Loss: 0.2993, Train: 0.5457, Val: 0.5473\n",
      "Epoch: 041, Loss: 0.2977, Train: 0.5443, Val: 0.5459\n",
      "Epoch: 042, Loss: 0.2962, Train: 0.5429, Val: 0.5445\n",
      "Epoch: 043, Loss: 0.2947, Train: 0.5415, Val: 0.5431\n",
      "Epoch: 044, Loss: 0.2932, Train: 0.5401, Val: 0.5417\n",
      "Epoch: 045, Loss: 0.2917, Train: 0.5387, Val: 0.5403\n",
      "Epoch: 046, Loss: 0.2902, Train: 0.5374, Val: 0.5390\n",
      "Epoch: 047, Loss: 0.2888, Train: 0.5360, Val: 0.5376\n",
      "Epoch: 048, Loss: 0.2873, Train: 0.5347, Val: 0.5363\n",
      "Epoch: 049, Loss: 0.2859, Train: 0.5334, Val: 0.5350\n",
      "Epoch: 050, Loss: 0.2845, Train: 0.5321, Val: 0.5337\n",
      "Epoch: 051, Loss: 0.2832, Train: 0.5309, Val: 0.5324\n",
      "Epoch: 052, Loss: 0.2818, Train: 0.5296, Val: 0.5312\n",
      "Epoch: 053, Loss: 0.2805, Train: 0.5284, Val: 0.5299\n",
      "Epoch: 054, Loss: 0.2792, Train: 0.5272, Val: 0.5287\n",
      "Epoch: 055, Loss: 0.2780, Train: 0.5260, Val: 0.5275\n",
      "Epoch: 056, Loss: 0.2767, Train: 0.5249, Val: 0.5264\n",
      "Epoch: 057, Loss: 0.2755, Train: 0.5238, Val: 0.5252\n",
      "Epoch: 058, Loss: 0.2743, Train: 0.5227, Val: 0.5241\n",
      "Epoch: 059, Loss: 0.2732, Train: 0.5216, Val: 0.5230\n",
      "Epoch: 060, Loss: 0.2720, Train: 0.5205, Val: 0.5219\n",
      "Epoch: 061, Loss: 0.2709, Train: 0.5195, Val: 0.5209\n",
      "Epoch: 062, Loss: 0.2699, Train: 0.5185, Val: 0.5198\n",
      "Epoch: 063, Loss: 0.2688, Train: 0.5175, Val: 0.5188\n",
      "Epoch: 064, Loss: 0.2678, Train: 0.5165, Val: 0.5179\n",
      "Epoch: 065, Loss: 0.2668, Train: 0.5156, Val: 0.5169\n",
      "Epoch: 066, Loss: 0.2658, Train: 0.5146, Val: 0.5160\n",
      "Epoch: 067, Loss: 0.2649, Train: 0.5137, Val: 0.5151\n",
      "Epoch: 068, Loss: 0.2639, Train: 0.5129, Val: 0.5142\n",
      "Epoch: 069, Loss: 0.2630, Train: 0.5120, Val: 0.5133\n",
      "Epoch: 070, Loss: 0.2622, Train: 0.5112, Val: 0.5125\n",
      "Epoch: 071, Loss: 0.2613, Train: 0.5104, Val: 0.5117\n",
      "Epoch: 072, Loss: 0.2605, Train: 0.5096, Val: 0.5109\n",
      "Epoch: 073, Loss: 0.2597, Train: 0.5088, Val: 0.5102\n",
      "Epoch: 074, Loss: 0.2589, Train: 0.5081, Val: 0.5095\n",
      "Epoch: 075, Loss: 0.2581, Train: 0.5073, Val: 0.5088\n",
      "Epoch: 076, Loss: 0.2574, Train: 0.5065, Val: 0.5082\n",
      "Epoch: 077, Loss: 0.2566, Train: 0.5057, Val: 0.5077\n",
      "Epoch: 078, Loss: 0.2557, Train: 0.5045, Val: 0.5077\n",
      "Epoch: 079, Loss: 0.2547, Train: 0.5028, Val: 0.5093\n",
      "Epoch: 080, Loss: 0.2535, Train: 0.5006, Val: 0.5133\n",
      "Epoch: 081, Loss: 0.2522, Train: 0.4990, Val: 0.5145\n",
      "Epoch: 082, Loss: 0.2500, Train: 0.4976, Val: 0.5129\n",
      "Epoch: 083, Loss: 0.2478, Train: 0.4968, Val: 0.5140\n",
      "Epoch: 084, Loss: 0.2468, Train: 0.4966, Val: 0.5089\n",
      "Epoch: 085, Loss: 0.2467, Train: 0.4956, Val: 0.5094\n",
      "Epoch: 086, Loss: 0.2456, Train: 0.4940, Val: 0.5133\n",
      "Epoch: 087, Loss: 0.2440, Train: 0.4922, Val: 0.5140\n",
      "Epoch: 088, Loss: 0.2423, Train: 0.4905, Val: 0.5163\n",
      "Epoch: 089, Loss: 0.2408, Train: 0.4886, Val: 0.5194\n",
      "Epoch: 090, Loss: 0.2393, Train: 0.4868, Val: 0.5172\n",
      "Epoch: 091, Loss: 0.2375, Train: 0.4852, Val: 0.5189\n",
      "Epoch: 092, Loss: 0.2359, Train: 0.4841, Val: 0.5180\n",
      "Epoch: 093, Loss: 0.2347, Train: 0.4830, Val: 0.5166\n",
      "Epoch: 094, Loss: 0.2336, Train: 0.4821, Val: 0.5190\n",
      "Epoch: 095, Loss: 0.2327, Train: 0.4815, Val: 0.5133\n",
      "Epoch: 096, Loss: 0.2320, Train: 0.4800, Val: 0.5187\n",
      "Epoch: 097, Loss: 0.2307, Train: 0.4785, Val: 0.5180\n",
      "Epoch: 098, Loss: 0.2293, Train: 0.4772, Val: 0.5152\n",
      "Epoch: 099, Loss: 0.2279, Train: 0.4759, Val: 0.5187\n",
      "Epoch: 100, Loss: 0.2268, Train: 0.4751, Val: 0.5188\n",
      "Epoch: 101, Loss: 0.2260, Train: 0.4745, Val: 0.5152\n",
      "Epoch: 102, Loss: 0.2254, Train: 0.4734, Val: 0.5169\n",
      "Epoch: 103, Loss: 0.2244, Train: 0.4723, Val: 0.5190\n",
      "Epoch: 104, Loss: 0.2235, Train: 0.4713, Val: 0.5156\n",
      "Epoch: 105, Loss: 0.2224, Train: 0.4703, Val: 0.5160\n",
      "Epoch: 106, Loss: 0.2214, Train: 0.4693, Val: 0.5181\n",
      "Epoch: 107, Loss: 0.2206, Train: 0.4683, Val: 0.5149\n",
      "Epoch: 108, Loss: 0.2196, Train: 0.4672, Val: 0.5158\n",
      "Epoch: 109, Loss: 0.2186, Train: 0.4661, Val: 0.5183\n",
      "Epoch: 110, Loss: 0.2178, Train: 0.4654, Val: 0.5145\n",
      "Epoch: 111, Loss: 0.2171, Train: 0.4642, Val: 0.5172\n",
      "Epoch: 112, Loss: 0.2162, Train: 0.4633, Val: 0.5171\n",
      "Epoch: 113, Loss: 0.2152, Train: 0.4625, Val: 0.5145\n",
      "Epoch: 114, Loss: 0.2143, Train: 0.4615, Val: 0.5176\n",
      "Epoch: 115, Loss: 0.2135, Train: 0.4607, Val: 0.5147\n",
      "Epoch: 116, Loss: 0.2126, Train: 0.4598, Val: 0.5160\n",
      "Epoch: 117, Loss: 0.2118, Train: 0.4589, Val: 0.5164\n",
      "Epoch: 118, Loss: 0.2110, Train: 0.4581, Val: 0.5148\n",
      "Epoch: 119, Loss: 0.2102, Train: 0.4570, Val: 0.5172\n",
      "Epoch: 120, Loss: 0.2095, Train: 0.4563, Val: 0.5146\n",
      "Epoch: 121, Loss: 0.2087, Train: 0.4552, Val: 0.5169\n",
      "Epoch: 122, Loss: 0.2078, Train: 0.4544, Val: 0.5151\n",
      "Epoch: 123, Loss: 0.2070, Train: 0.4536, Val: 0.5159\n",
      "Epoch: 124, Loss: 0.2063, Train: 0.4528, Val: 0.5153\n",
      "Epoch: 125, Loss: 0.2056, Train: 0.4519, Val: 0.5160\n",
      "Epoch: 126, Loss: 0.2048, Train: 0.4511, Val: 0.5149\n",
      "Epoch: 127, Loss: 0.2041, Train: 0.4501, Val: 0.5184\n",
      "Epoch: 128, Loss: 0.2035, Train: 0.4512, Val: 0.5096\n",
      "Epoch: 129, Loss: 0.2039, Train: 0.4504, Val: 0.5259\n",
      "Epoch: 130, Loss: 0.2045, Train: 0.4514, Val: 0.5069\n",
      "Epoch: 131, Loss: 0.2039, Train: 0.4494, Val: 0.5091\n",
      "Epoch: 132, Loss: 0.2023, Train: 0.4482, Val: 0.5252\n",
      "Epoch: 133, Loss: 0.2023, Train: 0.4460, Val: 0.5175\n",
      "Epoch: 134, Loss: 0.1996, Train: 0.4473, Val: 0.5091\n",
      "Epoch: 135, Loss: 0.2003, Train: 0.4454, Val: 0.5122\n",
      "Epoch: 136, Loss: 0.1987, Train: 0.4449, Val: 0.5220\n",
      "Epoch: 137, Loss: 0.1990, Train: 0.4433, Val: 0.5168\n",
      "Epoch: 138, Loss: 0.1972, Train: 0.4439, Val: 0.5110\n",
      "Epoch: 139, Loss: 0.1974, Train: 0.4424, Val: 0.5140\n",
      "Epoch: 140, Loss: 0.1962, Train: 0.4416, Val: 0.5212\n",
      "Epoch: 141, Loss: 0.1963, Train: 0.4406, Val: 0.5173\n",
      "Epoch: 142, Loss: 0.1950, Train: 0.4410, Val: 0.5126\n",
      "Epoch: 143, Loss: 0.1950, Train: 0.4397, Val: 0.5151\n",
      "Epoch: 144, Loss: 0.1940, Train: 0.4391, Val: 0.5204\n",
      "Epoch: 145, Loss: 0.1940, Train: 0.4384, Val: 0.5160\n",
      "Epoch: 146, Loss: 0.1929, Train: 0.4385, Val: 0.5131\n",
      "Epoch: 147, Loss: 0.1928, Train: 0.4372, Val: 0.5168\n",
      "Epoch: 148, Loss: 0.1919, Train: 0.4367, Val: 0.5197\n",
      "Epoch: 149, Loss: 0.1918, Train: 0.4363, Val: 0.5149\n",
      "Epoch: 150, Loss: 0.1911, Train: 0.4359, Val: 0.5146\n",
      "Epoch: 151, Loss: 0.1907, Train: 0.4349, Val: 0.5192\n",
      "Epoch: 152, Loss: 0.1902, Train: 0.4343, Val: 0.5179\n",
      "Epoch: 153, Loss: 0.1896, Train: 0.4343, Val: 0.5147\n",
      "Epoch: 154, Loss: 0.1893, Train: 0.4333, Val: 0.5175\n",
      "Epoch: 155, Loss: 0.1886, Train: 0.4327, Val: 0.5198\n",
      "Epoch: 156, Loss: 0.1884, Train: 0.4326, Val: 0.5158\n",
      "Epoch: 157, Loss: 0.1879, Train: 0.4319, Val: 0.5167\n",
      "Epoch: 158, Loss: 0.1874, Train: 0.4312, Val: 0.5201\n",
      "Epoch: 159, Loss: 0.1871, Train: 0.4309, Val: 0.5166\n",
      "Epoch: 160, Loss: 0.1866, Train: 0.4305, Val: 0.5167\n",
      "Epoch: 161, Loss: 0.1861, Train: 0.4298, Val: 0.5202\n",
      "Epoch: 162, Loss: 0.1859, Train: 0.4295, Val: 0.5172\n",
      "Epoch: 163, Loss: 0.1853, Train: 0.4289, Val: 0.5176\n",
      "Epoch: 164, Loss: 0.1849, Train: 0.4283, Val: 0.5204\n",
      "Epoch: 165, Loss: 0.1846, Train: 0.4282, Val: 0.5172\n",
      "Epoch: 166, Loss: 0.1842, Train: 0.4275, Val: 0.5185\n",
      "Epoch: 167, Loss: 0.1837, Train: 0.4270, Val: 0.5202\n",
      "Epoch: 168, Loss: 0.1835, Train: 0.4270, Val: 0.5171\n",
      "Epoch: 169, Loss: 0.1831, Train: 0.4261, Val: 0.5202\n",
      "Epoch: 170, Loss: 0.1827, Train: 0.4256, Val: 0.5198\n",
      "Epoch: 171, Loss: 0.1823, Train: 0.4256, Val: 0.5181\n",
      "Epoch: 172, Loss: 0.1820, Train: 0.4248, Val: 0.5214\n",
      "Epoch: 173, Loss: 0.1817, Train: 0.4247, Val: 0.5184\n",
      "Epoch: 174, Loss: 0.1813, Train: 0.4240, Val: 0.5206\n",
      "Epoch: 175, Loss: 0.1809, Train: 0.4236, Val: 0.5204\n",
      "Epoch: 176, Loss: 0.1805, Train: 0.4234, Val: 0.5193\n",
      "Epoch: 177, Loss: 0.1803, Train: 0.4228, Val: 0.5216\n",
      "Epoch: 178, Loss: 0.1800, Train: 0.4228, Val: 0.5190\n",
      "Epoch: 179, Loss: 0.1797, Train: 0.4220, Val: 0.5223\n",
      "Epoch: 180, Loss: 0.1794, Train: 0.4221, Val: 0.5193\n",
      "Epoch: 181, Loss: 0.1791, Train: 0.4213, Val: 0.5229\n",
      "Epoch: 182, Loss: 0.1788, Train: 0.4215, Val: 0.5195\n",
      "Epoch: 183, Loss: 0.1786, Train: 0.4206, Val: 0.5236\n",
      "Epoch: 184, Loss: 0.1783, Train: 0.4211, Val: 0.5191\n",
      "Epoch: 185, Loss: 0.1782, Train: 0.4201, Val: 0.5249\n",
      "Epoch: 186, Loss: 0.1781, Train: 0.4212, Val: 0.5183\n",
      "Epoch: 187, Loss: 0.1781, Train: 0.4196, Val: 0.5257\n",
      "Epoch: 188, Loss: 0.1777, Train: 0.4200, Val: 0.5195\n",
      "Epoch: 189, Loss: 0.1772, Train: 0.4185, Val: 0.5239\n",
      "Epoch: 190, Loss: 0.1765, Train: 0.4183, Val: 0.5227\n",
      "Epoch: 191, Loss: 0.1761, Train: 0.4184, Val: 0.5212\n",
      "Epoch: 192, Loss: 0.1760, Train: 0.4178, Val: 0.5253\n",
      "Epoch: 193, Loss: 0.1760, Train: 0.4186, Val: 0.5200\n",
      "Epoch: 194, Loss: 0.1761, Train: 0.4172, Val: 0.5261\n",
      "Epoch: 195, Loss: 0.1757, Train: 0.4175, Val: 0.5214\n",
      "Epoch: 196, Loss: 0.1752, Train: 0.4164, Val: 0.5245\n",
      "Epoch: 197, Loss: 0.1747, Train: 0.4161, Val: 0.5242\n",
      "Epoch: 198, Loss: 0.1744, Train: 0.4163, Val: 0.5225\n",
      "Epoch: 199, Loss: 0.1743, Train: 0.4156, Val: 0.5261\n",
      "Epoch: 200, Loss: 0.1743, Train: 0.4163, Val: 0.5217\n",
      "Epoch: 201, Loss: 0.1742, Train: 0.4151, Val: 0.5267\n",
      "Epoch: 202, Loss: 0.1740, Train: 0.4156, Val: 0.5225\n",
      "Epoch: 203, Loss: 0.1737, Train: 0.4144, Val: 0.5263\n",
      "Epoch: 204, Loss: 0.1732, Train: 0.4144, Val: 0.5243\n",
      "Epoch: 205, Loss: 0.1729, Train: 0.4140, Val: 0.5251\n",
      "Epoch: 206, Loss: 0.1726, Train: 0.4136, Val: 0.5259\n",
      "Epoch: 207, Loss: 0.1725, Train: 0.4139, Val: 0.5240\n",
      "Epoch: 208, Loss: 0.1724, Train: 0.4132, Val: 0.5274\n",
      "Epoch: 209, Loss: 0.1724, Train: 0.4142, Val: 0.5232\n",
      "Epoch: 210, Loss: 0.1725, Train: 0.4131, Val: 0.5293\n",
      "Epoch: 211, Loss: 0.1726, Train: 0.4149, Val: 0.5225\n",
      "Epoch: 212, Loss: 0.1729, Train: 0.4127, Val: 0.5297\n",
      "Epoch: 213, Loss: 0.1723, Train: 0.4131, Val: 0.5242\n",
      "Epoch: 214, Loss: 0.1716, Train: 0.4117, Val: 0.5271\n",
      "Epoch: 215, Loss: 0.1709, Train: 0.4115, Val: 0.5274\n",
      "Epoch: 216, Loss: 0.1708, Train: 0.4124, Val: 0.5245\n",
      "Epoch: 217, Loss: 0.1710, Train: 0.4114, Val: 0.5295\n",
      "Epoch: 218, Loss: 0.1711, Train: 0.4121, Val: 0.5248\n",
      "Epoch: 219, Loss: 0.1708, Train: 0.4106, Val: 0.5285\n",
      "Epoch: 220, Loss: 0.1702, Train: 0.4105, Val: 0.5274\n",
      "Epoch: 221, Loss: 0.1698, Train: 0.4107, Val: 0.5264\n",
      "Epoch: 222, Loss: 0.1698, Train: 0.4101, Val: 0.5295\n",
      "Epoch: 223, Loss: 0.1699, Train: 0.4109, Val: 0.5258\n",
      "Epoch: 224, Loss: 0.1699, Train: 0.4097, Val: 0.5296\n",
      "Epoch: 225, Loss: 0.1695, Train: 0.4098, Val: 0.5272\n",
      "Epoch: 226, Loss: 0.1691, Train: 0.4092, Val: 0.5284\n",
      "Epoch: 227, Loss: 0.1689, Train: 0.4089, Val: 0.5294\n",
      "Epoch: 228, Loss: 0.1688, Train: 0.4095, Val: 0.5273\n",
      "Epoch: 229, Loss: 0.1688, Train: 0.4087, Val: 0.5305\n",
      "Epoch: 230, Loss: 0.1688, Train: 0.4095, Val: 0.5271\n",
      "Epoch: 231, Loss: 0.1687, Train: 0.4083, Val: 0.5308\n",
      "Epoch: 232, Loss: 0.1685, Train: 0.4088, Val: 0.5278\n",
      "Epoch: 233, Loss: 0.1683, Train: 0.4078, Val: 0.5305\n",
      "Epoch: 234, Loss: 0.1680, Train: 0.4079, Val: 0.5290\n",
      "Epoch: 235, Loss: 0.1677, Train: 0.4075, Val: 0.5299\n",
      "Epoch: 236, Loss: 0.1675, Train: 0.4073, Val: 0.5301\n",
      "Epoch: 237, Loss: 0.1674, Train: 0.4075, Val: 0.5292\n",
      "Epoch: 238, Loss: 0.1673, Train: 0.4069, Val: 0.5310\n",
      "Epoch: 239, Loss: 0.1673, Train: 0.4076, Val: 0.5289\n",
      "Epoch: 240, Loss: 0.1673, Train: 0.4068, Val: 0.5324\n",
      "Epoch: 241, Loss: 0.1674, Train: 0.4085, Val: 0.5285\n",
      "Epoch: 242, Loss: 0.1678, Train: 0.4073, Val: 0.5343\n",
      "Epoch: 243, Loss: 0.1683, Train: 0.4106, Val: 0.5277\n",
      "Epoch: 244, Loss: 0.1693, Train: 0.4072, Val: 0.5348\n",
      "Epoch: 245, Loss: 0.1683, Train: 0.4074, Val: 0.5293\n",
      "Epoch: 246, Loss: 0.1670, Train: 0.4058, Val: 0.5312\n",
      "Epoch: 247, Loss: 0.1661, Train: 0.4059, Val: 0.5332\n",
      "Epoch: 248, Loss: 0.1667, Train: 0.4082, Val: 0.5288\n",
      "Epoch: 249, Loss: 0.1675, Train: 0.4057, Val: 0.5334\n",
      "Epoch: 250, Loss: 0.1665, Train: 0.4054, Val: 0.5312\n",
      "Epoch: 251, Loss: 0.1657, Train: 0.4060, Val: 0.5305\n",
      "Epoch: 252, Loss: 0.1659, Train: 0.4052, Val: 0.5340\n",
      "Epoch: 253, Loss: 0.1662, Train: 0.4060, Val: 0.5306\n",
      "Epoch: 254, Loss: 0.1659, Train: 0.4046, Val: 0.5325\n",
      "Epoch: 255, Loss: 0.1653, Train: 0.4045, Val: 0.5333\n",
      "Epoch: 256, Loss: 0.1654, Train: 0.4058, Val: 0.5309\n",
      "Epoch: 257, Loss: 0.1657, Train: 0.4043, Val: 0.5339\n",
      "Epoch: 258, Loss: 0.1653, Train: 0.4043, Val: 0.5324\n",
      "Epoch: 259, Loss: 0.1649, Train: 0.4043, Val: 0.5324\n",
      "Epoch: 260, Loss: 0.1648, Train: 0.4038, Val: 0.5345\n",
      "Epoch: 261, Loss: 0.1650, Train: 0.4048, Val: 0.5320\n",
      "Epoch: 262, Loss: 0.1650, Train: 0.4035, Val: 0.5342\n",
      "Epoch: 263, Loss: 0.1646, Train: 0.4035, Val: 0.5335\n",
      "Epoch: 264, Loss: 0.1643, Train: 0.4038, Val: 0.5329\n",
      "Epoch: 265, Loss: 0.1644, Train: 0.4031, Val: 0.5350\n",
      "Epoch: 266, Loss: 0.1644, Train: 0.4039, Val: 0.5329\n",
      "Epoch: 267, Loss: 0.1644, Train: 0.4028, Val: 0.5350\n",
      "Epoch: 268, Loss: 0.1641, Train: 0.4030, Val: 0.5340\n",
      "Epoch: 269, Loss: 0.1639, Train: 0.4028, Val: 0.5340\n",
      "Epoch: 270, Loss: 0.1638, Train: 0.4025, Val: 0.5351\n",
      "Epoch: 271, Loss: 0.1638, Train: 0.4032, Val: 0.5336\n",
      "Epoch: 272, Loss: 0.1639, Train: 0.4023, Val: 0.5359\n",
      "Epoch: 273, Loss: 0.1639, Train: 0.4032, Val: 0.5339\n",
      "Epoch: 274, Loss: 0.1638, Train: 0.4020, Val: 0.5361\n",
      "Epoch: 275, Loss: 0.1636, Train: 0.4026, Val: 0.5344\n",
      "Epoch: 276, Loss: 0.1634, Train: 0.4018, Val: 0.5357\n",
      "Epoch: 277, Loss: 0.1632, Train: 0.4020, Val: 0.5350\n",
      "Epoch: 278, Loss: 0.1631, Train: 0.4017, Val: 0.5354\n",
      "Epoch: 279, Loss: 0.1630, Train: 0.4015, Val: 0.5357\n",
      "Epoch: 280, Loss: 0.1629, Train: 0.4017, Val: 0.5353\n",
      "Epoch: 281, Loss: 0.1629, Train: 0.4012, Val: 0.5365\n",
      "Epoch: 282, Loss: 0.1629, Train: 0.4020, Val: 0.5353\n",
      "Epoch: 283, Loss: 0.1629, Train: 0.4012, Val: 0.5374\n",
      "Epoch: 284, Loss: 0.1631, Train: 0.4032, Val: 0.5349\n",
      "Epoch: 285, Loss: 0.1636, Train: 0.4019, Val: 0.5389\n",
      "Epoch: 286, Loss: 0.1642, Train: 0.4056, Val: 0.5350\n",
      "Epoch: 287, Loss: 0.1653, Train: 0.4021, Val: 0.5398\n",
      "Epoch: 288, Loss: 0.1646, Train: 0.4031, Val: 0.5356\n",
      "Epoch: 289, Loss: 0.1636, Train: 0.4005, Val: 0.5373\n",
      "Epoch: 290, Loss: 0.1622, Train: 0.4006, Val: 0.5375\n",
      "Epoch: 291, Loss: 0.1624, Train: 0.4032, Val: 0.5352\n",
      "Epoch: 292, Loss: 0.1635, Train: 0.4012, Val: 0.5386\n",
      "Epoch: 293, Loss: 0.1633, Train: 0.4015, Val: 0.5361\n",
      "Epoch: 294, Loss: 0.1624, Train: 0.4004, Val: 0.5371\n",
      "Epoch: 295, Loss: 0.1618, Train: 0.4003, Val: 0.5385\n",
      "Epoch: 296, Loss: 0.1624, Train: 0.4023, Val: 0.5361\n",
      "Epoch: 297, Loss: 0.1628, Train: 0.4001, Val: 0.5383\n",
      "Epoch: 298, Loss: 0.1621, Train: 0.4000, Val: 0.5373\n",
      "Epoch: 299, Loss: 0.1616, Train: 0.4006, Val: 0.5368\n",
      "Epoch: 300, Loss: 0.1618, Train: 0.3999, Val: 0.5387\n"
     ]
    }
   ],
   "source": [
    "########## GNN TRAINING ############\n",
    "#if model_mode == GNN run below\n",
    "model_gnn = GNN_recommender(data, train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GNN PRED FOR TEST_DATA ######### \n",
    "@torch.no_grad()\n",
    "def pred_gnn_gen(device, model, data):\n",
    "    data = data.to(device)\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict,\n",
    "                    data['user', 'review', 'item'].edge_attr,\n",
    "                    data['user', 'item'].edge_label_index)\n",
    "    # pred = pred.clamp(min=0, max=1)\n",
    "    target = data['user', 'item'].edge_label.float()\n",
    "    rmse = F.mse_loss(pred, target).sqrt()\n",
    "    return pred, target, float(rmse)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # To check the memory usage of GNN, put cpu to be comparable with MF models\n",
    "pred_gnn, ground_truth_gnn, rmse_gnn = pred_gnn_gen(device, model_gnn, test_data)\n",
    "pred_gnn = pred_gnn.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DATA PREPRATION FOR ML models #############\n",
    "'''\n",
    "For ML models, we need a df of train and test data, \n",
    "but from GNN train/test generation, we have a HeteroData\n",
    "Here we turn a HeteroData to a DataFrame: will cover in this section\n",
    "How to add review texts to each train and test?\n",
    "'''\n",
    "\n",
    "train_data_mask = torch.zeros(data[\"user\", \"review\", \"item\"].edge_index.shape[1], dtype=torch.bool)\n",
    "test_data_mask = torch.zeros(data[\"user\", \"review\", \"item\"].edge_index.shape[1], dtype=torch.bool)\n",
    "train_data_edges = set(tuple(edge) for edge in train_data[\"user\", \"review\", \"item\"].edge_label_index.t().tolist())\n",
    "test_data_edges = set(tuple(edge) for edge in test_data[\"user\", \"review\", \"item\"].edge_label_index.t().tolist())\n",
    "\n",
    "for i, edge in enumerate(data[\"user\", \"review\", \"item\"].edge_index.t().tolist()):\n",
    "    edge_tuple = tuple(edge)\n",
    "    if edge_tuple in train_data_edges:\n",
    "        train_data_mask[i] = True\n",
    "    elif edge_tuple in test_data_edges:\n",
    "        test_data_mask[i] = True\n",
    "\n",
    "test_df_index = test_data['user', 'review', 'item'].edge_label_index.cpu().numpy()\n",
    "test_df_label = test_data['user', 'review', 'item'].edge_label.cpu().numpy()\n",
    "\n",
    "test_df_index = test_df_index.T \n",
    "test_df_ml = pd.DataFrame(test_df_index, columns=['user', 'item'])\n",
    "test_df_ml['review_label'] = test_df_label\n",
    "test_df_ml['review_text'] = review_ratings_df.loc[test_data_mask.numpy(), 'review_text'].values\n",
    "\n",
    "train_df_index = train_data['user', 'review', 'item'].edge_label_index.cpu().numpy()\n",
    "train_df_label = train_data['user', 'review', 'item'].edge_label.cpu().numpy()\n",
    "train_df_index = train_df_index.T \n",
    "train_df_ml = pd.DataFrame(train_df_index, columns=['user', 'item'])\n",
    "train_df_ml['review_label'] = train_df_label\n",
    "train_df_ml['review_text'] = review_ratings_df.loc[train_data_mask.numpy(), 'review_text'].values\n",
    "\n",
    "if experiment == 'word2vec':\n",
    "    edge_feat_df = pd.DataFrame(edge_feat.cpu().numpy())\n",
    "    test_df_ml['review_word2vec'] = edge_feat_df.loc[test_data_mask.numpy(), 'review_word2vec'].values\n",
    "    train_df_ml['review_word2vec'] = edge_feat_df.loc[train_data_mask.numpy(), 'review_word2vec'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models: 100%|██████████| 4/4 [21:43<00:00, 325.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine accuracy {'pred_Logistic Regression': 0.5908452118048585, 'pred_SVM': 0.590443686006826, 'pred_Decision Trees': 0.5312186307970287, 'pred_Random Forest': 0.5824131700461754, 'pred_GNN': 0.503111824934752}\n",
      "accuracy for class 1 {'pred_Logistic Regression': 0.0, 'pred_SVM': 0.004416094210009814, 'pred_Decision Trees': 0.4210009813542689, 'pred_Random Forest': 0.047105004906771344, 'pred_GNN': 0.3621197252208047}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "######### TRAIN & PRED ML MODELS #########\n",
    "train_df_ml['review_text'] = train_df_ml['review_text'].apply(lambda x: x[:512])\n",
    "test_df_ml['review_text'] = test_df_ml['review_text'].apply(lambda x: x[:512])\n",
    "\n",
    "if experiment == 'TFIDF' or experiment == 'SBERT':\n",
    "    vectorizer = TfidfVectorizer(max_features=embedding_size)\n",
    "    X_train = vectorizer.fit_transform(train_df_ml['review_text'])\n",
    "    X_test = vectorizer.transform(test_df_ml['review_text'])\n",
    "\n",
    "if experiment == 'word2vec':\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "y_train = train_df_ml['review_label']\n",
    "y_test = test_df_ml['review_label']\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Decision Trees\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    # \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    # \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    # \"Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Training models\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions[f\"pred_{name}\"] = model.predict(X_test)\n",
    "\n",
    "# TODO: add the GNN results to the predictions dict\n",
    "predictions['pred_GNN'] = [1 if pred > 0.5 else 0 for pred in pred_gnn]\n",
    "\n",
    "accuracies = {}\n",
    "for name, pred in predictions.items():\n",
    "    accuracies[name] = accuracy_score(y_test, pred)\n",
    "\n",
    "print('combine accuracy', accuracies)   \n",
    "\n",
    "accuracies_class_0 = {}\n",
    "\n",
    "for name, pred in predictions.items():\n",
    "    y_test_array = np.array(y_test)\n",
    "    pred_array = np.array(pred)\n",
    "    \n",
    "    class_ = 1\n",
    "    mask = y_test_array == class_\n",
    "    accuracies_class_0[name] = accuracy_score(y_test_array[mask], pred_array[mask])\n",
    "\n",
    "print(f'accuracy for class {class_}', accuracies_class_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "pred_Logistic Regression: 0.5908\n",
      "pred_SVM: 0.5904\n",
      "pred_Decision Trees: 0.5312\n",
      "pred_Random Forest: 0.5824\n",
      "pred_GNN: 0.5031\n",
      "\n",
      "Precision:\n",
      "pred_Logistic Regression: 0.0000\n",
      "pred_SVM: 0.4500\n",
      "pred_Decision Trees: 0.4262\n",
      "pred_Random Forest: 0.4103\n",
      "pred_GNN: 0.3858\n",
      "\n",
      "Recall:\n",
      "pred_Logistic Regression: 0.0000\n",
      "pred_SVM: 0.0044\n",
      "pred_Decision Trees: 0.4210\n",
      "pred_Random Forest: 0.0471\n",
      "pred_GNN: 0.3621\n",
      "\n",
      "F1 Score:\n",
      "pred_Logistic Regression: 0.0000\n",
      "pred_SVM: 0.0087\n",
      "pred_Decision Trees: 0.4236\n",
      "pred_Random Forest: 0.0845\n",
      "pred_GNN: 0.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seankhatiri/Fakeclub/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "####### METRIC EVAL #######\n",
    "eval_metrics = {\n",
    "    'Accuracy': accuracy_score,\n",
    "    'Precision': precision_score,\n",
    "    'Recall': recall_score,\n",
    "    'F1 Score': f1_score\n",
    "}\n",
    "\n",
    "results = {metric: {} for metric in eval_metrics}\n",
    "\n",
    "for name, pred in predictions.items():\n",
    "    for metric_name, metric_func in eval_metrics.items():\n",
    "        results[metric_name][name] = metric_func(y_test, pred)\n",
    "\n",
    "for metric_name, model_results in results.items():\n",
    "    print(f\"\\n{metric_name}:\")\n",
    "    for model_name, value in model_results.items():\n",
    "        print(f\"{model_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# DIVERSITY EXPERIMENT ##############\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
